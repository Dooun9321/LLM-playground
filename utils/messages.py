"""
LangChain/LangGraph ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë° ë° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

ìµœì‹  LangChain v1.0+ ë° LangGraph íŒ¨í„´ì„ ì§€ì›í•©ë‹ˆë‹¤.

Note:
    - LangChain v1.0ì—ì„œ ToolAgentAction, AgentAction, AgentStep ë“±ì€ deprecatedë¨
    - ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ëŠ” create_agent ë° tool_calls íŒ¨í„´ì„ ì‚¬ìš©
"""
from langchain_core.messages import AIMessageChunk
from typing import Any, Dict, List, Callable, Optional, Union, Literal, AsyncIterator
from dataclasses import dataclass
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph.state import CompiledStateGraph
import uuid


# ============================================================
# ìƒìˆ˜ ì •ì˜
# ============================================================
SEPARATOR_WIDTH = 50
SEPARATOR_LINE = "=" * SEPARATOR_WIDTH
SEPARATOR_DASH = "- " * 25

# ANSI ìƒ‰ìƒ ì½”ë“œ
ANSI_RESET = "\033[0m"
ANSI_CYAN = "\033[1;36m"
ANSI_YELLOW = "\033[1;33m"
ANSI_GREEN = "\033[1;32m"
ANSI_MAGENTA = "\033[1;35m"

# ê° ê¹Šì´ ìˆ˜ì¤€ì— ëŒ€í•´ ë¯¸ë¦¬ ì •ì˜ëœ ìƒ‰ìƒ (ANSI ì´ìŠ¤ì¼€ì´í”„ ì½”ë“œ ì‚¬ìš©)
DEPTH_COLORS = {
    1: "\033[96m",  # ë°ì€ ì²­ë¡ìƒ‰ (ëˆˆì— ì˜ ë„ëŠ” ì²« ê³„ì¸µ)
    2: "\033[93m",  # ë…¸ë€ìƒ‰ (ë‘ ë²ˆì§¸ ê³„ì¸µ)
    3: "\033[94m",  # ë°ì€ íŒŒë€ìƒ‰ (ì„¸ ë²ˆì§¸ ê³„ì¸µ)
    4: "\033[95m",  # ë³´ë¼ìƒ‰ (ë„¤ ë²ˆì§¸ ê³„ì¸µ)
    5: "\033[92m",  # ë°ì€ ì´ˆë¡ìƒ‰ (ë‹¤ì„¯ ë²ˆì§¸ ê³„ì¸µ)
    "default": "\033[96m",
    "reset": "\033[0m",
}

# ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ íƒ€ì…
StreamMode = Literal["messages", "updates", "values"]


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def random_uuid() -> str:
    """ëœë¤ UUID ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return str(uuid.uuid4())


def get_role_from_messages(msg: BaseMessage) -> str:
    """ë©”ì‹œì§€ ê°ì²´ì—ì„œ ì—­í• (role)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if isinstance(msg, HumanMessage):
        return "user"
    elif isinstance(msg, AIMessage):
        return "assistant"
    else:
        return "assistant"


def messages_to_history(messages: List[BaseMessage]) -> str:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ íˆìŠ¤í† ë¦¬ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return "\n".join(
        [f"{get_role_from_messages(msg)}: {msg.content}" for msg in messages]
    )


def format_namespace(namespace: tuple) -> str:
    """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    return namespace[-1].split(":")[0] if len(namespace) > 0 else "root graph"


# ============================================================
# ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹° (ìµœì‹  LangChain v1.0+ íŒ¨í„´ ì§€ì›)
# ============================================================
@dataclass
class ParsedContentBlock:
    """íŒŒì‹±ëœ content_block ì •ë³´"""
    block_type: str  # "text", "reasoning", "tool_call", "tool_result", etc.
    content: Any
    metadata: Optional[Dict[str, Any]] = None


def parse_content_blocks(msg: Union[AIMessageChunk, BaseMessage, Any]) -> List[ParsedContentBlock]:
    """
    ë©”ì‹œì§€ì˜ content_blocksë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    
    LangChain v1.0+ì˜ í†µí•© content_blocks í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤:
    - type: "text" - í…ìŠ¤íŠ¸ ì‘ë‹µ
    - type: "reasoning" - ëª¨ë¸ì˜ ì¶”ë¡  ê³¼ì •
    - type: "tool_call" - ë„êµ¬ í˜¸ì¶œ
    - type: "tool_result" - ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
    
    Args:
        msg: ë©”ì‹œì§€ ê°ì²´
        
    Returns:
        List[ParsedContentBlock]: íŒŒì‹±ëœ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
    """
    blocks = []
    
    # content_blocks ì†ì„± í™•ì¸ (ìµœì‹  íŒ¨í„´)
    if hasattr(msg, "content_blocks") and msg.content_blocks:
        for block in msg.content_blocks:
            if isinstance(block, dict):
                block_type = block.get("type", "text")
                if block_type == "text":
                    blocks.append(ParsedContentBlock(
                        block_type="text",
                        content=block.get("text", "")
                    ))
                elif block_type == "reasoning":
                    blocks.append(ParsedContentBlock(
                        block_type="reasoning",
                        content=block.get("reasoning", "")
                    ))
                elif block_type == "tool_call":
                    blocks.append(ParsedContentBlock(
                        block_type="tool_call",
                        content=block.get("args", {}),
                        metadata={"name": block.get("name"), "id": block.get("id")}
                    ))
                else:
                    blocks.append(ParsedContentBlock(
                        block_type=block_type,
                        content=block
                    ))
            elif hasattr(block, "text"):
                blocks.append(ParsedContentBlock(
                    block_type="text",
                    content=block.text
                ))
    
    return blocks


def extract_message_text(msg: Union[AIMessageChunk, BaseMessage, Any]) -> str:
    """
    ë©”ì‹œì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ìµœì‹  LangChain v1.0+ì˜ .text ì†ì„±ê³¼ content_blocksë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        msg: AIMessageChunk, BaseMessage, ë˜ëŠ” ê¸°íƒ€ ë©”ì‹œì§€ ê°ì²´
        
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    # ìµœì‹  LangChain v1.0+: .text ì†ì„± (ê¶Œì¥)
    if hasattr(msg, "text") and msg.text:
        return msg.text
    
    # ìµœì‹  LangChain v1.0+: content_blocks ì†ì„±
    if hasattr(msg, "content_blocks") and msg.content_blocks:
        texts = []
        for block in msg.content_blocks:
            if isinstance(block, dict):
                block_type = block.get("type", "text")
                if block_type == "text":
                    texts.append(block.get("text", ""))
                # reasoning íƒ€ì…ì€ ë³„ë„ ì²˜ë¦¬ (ì˜µì…˜)
            elif hasattr(block, "text"):
                texts.append(block.text)
        if texts:
            return "".join(texts)
    
    # ê¸°ì¡´ ë°©ì‹: content ì†ì„±
    if hasattr(msg, "content"):
        content = msg.content
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            texts = []
            for item in content:
                if isinstance(item, dict) and "text" in item:
                    texts.append(item["text"])
                elif isinstance(item, str):
                    texts.append(item)
            return "".join(texts)
    
    # ë¬¸ìì—´ì¸ ê²½ìš°
    if isinstance(msg, str):
        return msg
    
    return ""


def extract_reasoning(msg: Union[AIMessageChunk, BaseMessage, Any]) -> Optional[str]:
    """
    ë©”ì‹œì§€ì—ì„œ ì¶”ë¡ (reasoning) ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    LangChain v1.0+ì—ì„œ ì¼ë¶€ ëª¨ë¸ì€ reasoning ë¸”ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        msg: ë©”ì‹œì§€ ê°ì²´
        
    Returns:
        Optional[str]: ì¶”ë¡  ë‚´ìš© ë˜ëŠ” None
    """
    if hasattr(msg, "content_blocks") and msg.content_blocks:
        for block in msg.content_blocks:
            if isinstance(block, dict) and block.get("type") == "reasoning":
                return block.get("reasoning", "")
    return None


def extract_tool_calls(msg: Union[AIMessageChunk, BaseMessage, Any]) -> List[Dict[str, Any]]:
    """
    ë©”ì‹œì§€ì—ì„œ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    LangChain v1.0+ì˜ tool_calls ë° content_blocks íŒ¨í„´ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    
    Args:
        msg: ë©”ì‹œì§€ ê°ì²´
        
    Returns:
        List[Dict[str, Any]]: ë„êµ¬ í˜¸ì¶œ ë¦¬ìŠ¤íŠ¸
    """
    tool_calls = []
    
    # ë°©ë²• 1: tool_calls ì†ì„± (í‘œì¤€)
    if hasattr(msg, "tool_calls") and msg.tool_calls:
        tool_calls = list(msg.tool_calls)
    
    # ë°©ë²• 2: content_blocksì—ì„œ tool_call íƒ€ì… ì¶”ì¶œ
    if not tool_calls and hasattr(msg, "content_blocks") and msg.content_blocks:
        for block in msg.content_blocks:
            if isinstance(block, dict) and block.get("type") == "tool_call":
                tool_calls.append({
                    "name": block.get("name"),
                    "args": block.get("args", {}),
                    "id": block.get("id"),
                })
    
    return tool_calls


# ============================================================
# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
# ============================================================
def stream_response(response, return_output: bool = False) -> Optional[str]:
    """
    AI ëª¨ë¸ë¡œë¶€í„°ì˜ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ê° ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ë©´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    ìµœì‹  LangChain íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ì²­í¬ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤.

    Args:
        response (iterable): AIMessageChunk ê°ì²´ ë˜ëŠ” ë¬¸ìì—´ì˜ ì´í„°ëŸ¬ë¸”
        return_output (bool): Trueì¸ ê²½ìš° ì—°ê²°ëœ ì‘ë‹µ ë¬¸ìì—´ ë°˜í™˜

    Returns:
        Optional[str]: return_outputì´ Trueì¸ ê²½ìš° ì—°ê²°ëœ ì‘ë‹µ ë¬¸ìì—´
    """
    # ìµœì‹  LangChain íŒ¨í„´: ì²­í¬ ëˆ„ì 
    full_message: Optional[AIMessageChunk] = None
    answer = ""
    
    for chunk in response:
        if isinstance(chunk, AIMessageChunk):
            # ìµœì‹  íŒ¨í„´: ì²­í¬ í•©ì‚°
            full_message = chunk if full_message is None else full_message + chunk
            text = extract_message_text(chunk)
            if text:
                answer += text
                print(text, end="", flush=True)
        elif isinstance(chunk, str):
            answer += chunk
            print(chunk, end="", flush=True)
    
    if return_output:
        return answer
    return None


async def astream_response(
    response: AsyncIterator, 
    return_output: bool = False
) -> Optional[str]:
    """
    AI ëª¨ë¸ë¡œë¶€í„°ì˜ ì‘ë‹µì„ ë¹„ë™ê¸°ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

    Args:
        response: AsyncIterator of AIMessageChunk or strings
        return_output (bool): Trueì¸ ê²½ìš° ì—°ê²°ëœ ì‘ë‹µ ë¬¸ìì—´ ë°˜í™˜

    Returns:
        Optional[str]: return_outputì´ Trueì¸ ê²½ìš° ì—°ê²°ëœ ì‘ë‹µ ë¬¸ìì—´
    """
    full_message: Optional[AIMessageChunk] = None
    answer = ""
    
    async for chunk in response:
        if isinstance(chunk, AIMessageChunk):
            full_message = chunk if full_message is None else full_message + chunk
            text = extract_message_text(chunk)
            if text:
                answer += text
                print(text, end="", flush=True)
        elif isinstance(chunk, str):
            answer += chunk
            print(chunk, end="", flush=True)
    
    if return_output:
        return answer
    return None


# ============================================================
# ì½œë°± í•¨ìˆ˜ ë° í´ë˜ìŠ¤
# ============================================================
def tool_callback(tool: Dict[str, Any]) -> None:
    """ë„êµ¬ í˜¸ì¶œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì½œë°± í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print(f"\n{ANSI_MAGENTA}[ë„êµ¬ í˜¸ì¶œ]{ANSI_RESET}")
    tool_name = tool.get("tool") or tool.get("name", "unknown")
    print(f"Tool: {tool_name}")
    if tool_id := tool.get("id"):
        print(f"ID: {tool_id}")
    if tool_input := tool.get("tool_input") or tool.get("args"):
        if isinstance(tool_input, dict):
            for k, v in tool_input.items():
                print(f"  {k}: {v}")
        else:
            print(f"  Input: {tool_input}")


def observation_callback(observation: Dict[str, Any]) -> None:
    """ê´€ì°° ê²°ê³¼(ë„êµ¬ ì‹¤í–‰ ê²°ê³¼)ë¥¼ ì¶œë ¥í•˜ëŠ” ì½œë°± í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print(f"\n{ANSI_YELLOW}[ë„êµ¬ ê²°ê³¼]{ANSI_RESET}")
    if name := observation.get("name"):
        print(f"Tool: {name}")
    content = observation.get("observation", "")
    # ê¸´ ê²°ê³¼ëŠ” ì˜ë¼ì„œ í‘œì‹œ
    if len(str(content)) > 500:
        print(f"Result: {str(content)[:500]}...")
    else:
        print(f"Result: {content}")


def result_callback(result: str) -> None:
    """ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì½œë°± í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print(f"\n{ANSI_GREEN}[ìµœì¢… ë‹µë³€]{ANSI_RESET}")
    print(result)


@dataclass
class AgentCallbacks:
    """
    ì—ì´ì „íŠ¸ ì½œë°± í•¨ìˆ˜ë“¤ì„ í¬í•¨í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

    Attributes:
        tool_callback: ë„êµ¬ ì‚¬ìš© ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜
        observation_callback: ê´€ì°° ê²°ê³¼ ì²˜ë¦¬ ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜
        result_callback: ìµœì¢… ê²°ê³¼ ì²˜ë¦¬ ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜
    """
    tool_callback: Callable[[Dict[str, Any]], None] = tool_callback
    observation_callback: Callable[[Dict[str, Any]], None] = observation_callback
    result_callback: Callable[[str], None] = result_callback


# ============================================================
# ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¼ íŒŒì„œ (ìµœì‹  LangChain v1.0+ íŒ¨í„´)
# ============================================================
class AgentStreamParser:
    """
    ì—ì´ì „íŠ¸ì˜ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ì„ íŒŒì‹±í•˜ê³  ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    LangChain v1.0+ì—ì„œëŠ” tool_calls ê¸°ë°˜ íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Example:
        ```python
        parser = AgentStreamParser()
        for chunk in agent.stream(inputs, stream_mode="updates"):
            for node_name, node_output in chunk.items():
                parser.process_node_output(node_name, node_output)
        ```
    """

    def __init__(self, callbacks: Optional[AgentCallbacks] = None):
        """
        AgentStreamParser ê°ì²´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            callbacks: íŒŒì‹± ê³¼ì •ì—ì„œ ì‚¬ìš©í•  ì½œë°± í•¨ìˆ˜ë“¤
        """
        self.callbacks = callbacks or AgentCallbacks()
        self.output: Optional[str] = None

    def process_node_output(self, node_name: str, node_output: Any) -> None:
        """
        ë…¸ë“œ ì¶œë ¥ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ìµœì‹  LangGraph íŒ¨í„´).
        
        Args:
            node_name: ë…¸ë“œ ì´ë¦„
            node_output: ë…¸ë“œ ì¶œë ¥ê°’
        """
        if isinstance(node_output, dict):
            messages = node_output.get("messages", [])
            if isinstance(messages, list):
                for msg in messages:
                    self._process_message(msg)
            elif messages:
                self._process_message(messages)

    def _process_message(self, msg: Any) -> None:
        """ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # AI ë©”ì‹œì§€ì˜ tool_calls ì²˜ë¦¬
        if isinstance(msg, (AIMessage, AIMessageChunk)):
            tool_calls = extract_tool_calls(msg)
            if tool_calls:
                for tc in tool_calls:
                    self._process_tool_call(tc)
            else:
                # ìµœì¢… ì‘ë‹µ
                text = extract_message_text(msg)
                if text:
                    self._process_result(text)
        
        # Tool ë©”ì‹œì§€ (ë„êµ¬ ì‹¤í–‰ ê²°ê³¼) ì²˜ë¦¬
        elif isinstance(msg, ToolMessage):
            self._process_observation(msg)

    def _process_tool_call(self, tool_call: Dict[str, Any]) -> None:
        """ë„êµ¬ í˜¸ì¶œì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        tool_action = {
            "tool": tool_call.get("name"),
            "tool_input": tool_call.get("args"),
            "id": tool_call.get("id"),
        }
        self.callbacks.tool_callback(tool_action)

    def _process_observation(self, tool_msg: ToolMessage) -> None:
        """ë„êµ¬ ì‹¤í–‰ ê²°ê³¼(ê´€ì°°)ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        observation_dict = {
            "observation": tool_msg.content,
            "tool_call_id": getattr(tool_msg, "tool_call_id", None),
            "name": getattr(tool_msg, "name", None),
        }
        self.callbacks.observation_callback(observation_dict)

    def _process_result(self, result: str) -> None:
        """ìµœì¢… ê²°ê³¼ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        self.callbacks.result_callback(result)
        self.output = result

    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë ˆê±°ì‹œ ë©”ì„œë“œ
    def process_agent_steps(self, step: Dict[str, Any]) -> None:
        """
        ì—ì´ì „íŠ¸ì˜ ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Note: ì´ ë©”ì„œë“œëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë©ë‹ˆë‹¤.
              ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” process_node_outputì„ ì‚¬ìš©í•˜ì„¸ìš”.
        """
        if "messages" in step:
            messages = step["messages"]
            if isinstance(messages, list):
                for msg in messages:
                    self._process_message(msg)
            else:
                self._process_message(messages)
        elif "output" in step:
            self._process_result(step["output"])


# ============================================================
# ë©”ì‹œì§€ ì¶œë ¥ ìœ í‹¸ë¦¬í‹°
# ============================================================
def pretty_print_messages(messages: List[BaseMessage]) -> None:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì˜ˆì˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    for message in messages:
        message.pretty_print()


def is_terminal_dict(data: Any) -> bool:
    """ë§ë‹¨ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not isinstance(data, dict):
        return False
    for value in data.values():
        if isinstance(value, (dict, list)) or hasattr(value, "__dict__"):
            return False
    return True


def format_terminal_dict(data: Dict[str, Any]) -> str:
    """ë§ë‹¨ ë”•ì…”ë„ˆë¦¬ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    items = []
    for key, value in data.items():
        if isinstance(value, str):
            items.append(f'"{key}": "{value}"')
        else:
            items.append(f'"{key}": {value}')
    return "{" + ", ".join(items) + "}"


def _display_message_tree(
    data: Any, indent: int = 0, node: Optional[str] = None, is_root: bool = False
) -> None:
    """JSON ê°ì²´ì˜ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ íƒ€ì… ì •ë³´ ì—†ì´ ì¶œë ¥í•©ë‹ˆë‹¤."""
    spacing = " " * indent * 4
    color = DEPTH_COLORS.get(indent + 1, DEPTH_COLORS["default"])

    if isinstance(data, dict):
        if not is_root and node is not None:
            if is_terminal_dict(data):
                print(
                    f'{spacing}{color}{node}{DEPTH_COLORS["reset"]}: {format_terminal_dict(data)}'
                )
            else:
                print(f'{spacing}{color}{node}{DEPTH_COLORS["reset"]}:')
                for key, value in data.items():
                    _display_message_tree(value, indent + 1, key)
        else:
            for key, value in data.items():
                _display_message_tree(value, indent + 1, key)

    elif isinstance(data, list):
        if not is_root and node is not None:
            print(f'{spacing}{color}{node}{DEPTH_COLORS["reset"]}:')

        for index, item in enumerate(data):
            print(f'{spacing}    {color}index [{index}]{DEPTH_COLORS["reset"]}')
            _display_message_tree(item, indent + 1)

    elif hasattr(data, "__dict__") and not is_root:
        if node is not None:
            print(f'{spacing}{color}{node}{DEPTH_COLORS["reset"]}:')
        _display_message_tree(data.__dict__, indent)

    else:
        if node is not None:
            value_str = f'"{data}"' if isinstance(data, str) else str(data)
            print(f'{spacing}{color}{node}{DEPTH_COLORS["reset"]}: {value_str}')


def display_message_tree(message: Union[BaseMessage, Any]) -> None:
    """ë©”ì‹œì§€ íŠ¸ë¦¬ë¥¼ í‘œì‹œí•˜ëŠ” ì£¼ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    if isinstance(message, BaseMessage):
        _display_message_tree(message.__dict__, is_root=True)
    else:
        _display_message_tree(message, is_root=True)


# ============================================================
# Message Chunk Accumulator (ìµœì‹  LangChain íŒ¨í„´)
# ============================================================
class MessageChunkAccumulator:
    """
    ë©”ì‹œì§€ ì²­í¬ë¥¼ ëˆ„ì í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ìµœì‹  LangChain íŒ¨í„´ì— ë”°ë¼ AIMessageChunkë¥¼ í•©ì‚°í•©ë‹ˆë‹¤.
    
    Example:
        ```python
        accumulator = MessageChunkAccumulator()
        for chunk in model.stream("Hello"):
            accumulator.add(chunk)
            print(accumulator.text)  # ëˆ„ì ëœ í…ìŠ¤íŠ¸
        full_message = accumulator.get_full_message()
        ```
    """

    def __init__(self):
        self._reset_state()

    def _reset_state(self) -> None:
        """ìƒíƒœ ì´ˆê¸°í™”"""
        self.gathered: Optional[AIMessageChunk] = None
        self.current_node: Optional[str] = None
        self.current_namespace: Optional[str] = None

    def add(
        self,
        chunk: AIMessageChunk,
        node: Optional[str] = None,
        namespace: Optional[str] = None,
    ) -> None:
        """
        ì²­í¬ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤.
        
        Args:
            chunk: ì¶”ê°€í•  AI ë©”ì‹œì§€ ì²­í¬
            node: í˜„ì¬ ë…¸ë“œëª… (ì„ íƒì‚¬í•­)
            namespace: í˜„ì¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ì„ íƒì‚¬í•­)
        """
        # ë…¸ë“œ/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ë³€ê²½ë˜ë©´ ë¦¬ì…‹
        if self._should_reset(node, namespace):
            self._reset_state()

        self.current_node = node if node is not None else self.current_node
        self.current_namespace = namespace if namespace is not None else self.current_namespace

        # ìµœì‹  LangChain íŒ¨í„´: ì²­í¬ í•©ì‚°
        self.gathered = chunk if self.gathered is None else self.gathered + chunk

    def _should_reset(self, node: Optional[str], namespace: Optional[str]) -> bool:
        """ìƒíƒœ ë¦¬ì…‹ ì—¬ë¶€ í™•ì¸"""
        if node is None and namespace is None:
            return False
        if node is not None and self.current_node is not None and node != self.current_node:
            return True
        if namespace is not None and self.current_namespace is not None and namespace != self.current_namespace:
            return True
        return False

    @property
    def text(self) -> str:
        """í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.gathered is None:
            return ""
        return extract_message_text(self.gathered)

    @property
    def tool_calls(self) -> List[Dict[str, Any]]:
        """í˜„ì¬ê¹Œì§€ ëˆ„ì ëœ ë„êµ¬ í˜¸ì¶œì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.gathered is None:
            return []
        return extract_tool_calls(self.gathered)

    def get_full_message(self) -> Optional[AIMessageChunk]:
        """ì „ì²´ ëˆ„ì ëœ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.gathered

    def reset(self) -> None:
        """ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self._reset_state()


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ToolChunkHandler = MessageChunkAccumulator


# ============================================================
# ì¶œë ¥ í—¬í¼ í•¨ìˆ˜
# ============================================================
def _print_node_header(
    node_name: str, namespace: Optional[tuple] = None, prev_node: str = ""
) -> None:
    """ë…¸ë“œ í—¤ë”ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    if node_name == prev_node:
        return
    
    print("\n" + SEPARATOR_LINE)
    if namespace is None or format_namespace(namespace) == "root graph":
        print(f"ğŸ”„ Node: {ANSI_CYAN}{node_name}{ANSI_RESET} ğŸ”„")
    else:
        formatted_namespace = format_namespace(namespace)
        print(
            f"ğŸ”„ Node: {ANSI_CYAN}{node_name}{ANSI_RESET} in [{ANSI_YELLOW}{formatted_namespace}{ANSI_RESET}] ğŸ”„"
        )
    print(SEPARATOR_DASH)


def _print_chunk_content(chunk_msg: Any, show_reasoning: bool = False) -> None:
    """
    ì²­í¬ ë©”ì‹œì§€ì˜ ë‚´ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        chunk_msg: ì¶œë ¥í•  ì²­í¬ ë©”ì‹œì§€
        show_reasoning: reasoning ë¸”ë¡ í‘œì‹œ ì—¬ë¶€ (ê¸°ë³¸: False)
    """
    # Reasoning ì¶œë ¥ (ì˜µì…˜)
    if show_reasoning:
        reasoning = extract_reasoning(chunk_msg)
        if reasoning:
            print(f"{ANSI_YELLOW}[Reasoning]{ANSI_RESET} {reasoning}", end="", flush=True)
    
    # í…ìŠ¤íŠ¸ ì¶œë ¥
    text = extract_message_text(chunk_msg)
    if text:
        print(text, end="", flush=True)
    
    # ë„êµ¬ í˜¸ì¶œ ì •ë³´ ì¶œë ¥
    tool_calls = extract_tool_calls(chunk_msg)
    if tool_calls:
        for tc in tool_calls:
            tool_name = tc.get("name", "unknown")
            print(f"\n{ANSI_MAGENTA}[Tool Call]{ANSI_RESET} {tool_name}", end="", flush=True)


def _print_base_message(msg: BaseMessage, streaming: bool = True) -> None:
    """BaseMessageë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    text = extract_message_text(msg)
    if text:
        if streaming:
            print(text, end="", flush=True)
        else:
            print(text)
    else:
        msg.pretty_print()


def _print_node_chunk(node_chunk: Any, streaming: bool = True) -> None:
    """
    ë…¸ë“œ ì²­í¬ ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        node_chunk: ì¶œë ¥í•  ë…¸ë“œ ì²­í¬
        streaming: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì—¬ë¶€ (Trueë©´ flush ì‚¬ìš©)
    """
    if node_chunk is None:
        return

    if isinstance(node_chunk, dict):
        for k, v in node_chunk.items():
            if isinstance(v, BaseMessage):
                _print_base_message(v, streaming)
            elif isinstance(v, list):
                for list_item in v:
                    if isinstance(list_item, BaseMessage):
                        _print_base_message(list_item, streaming)
                    else:
                        text = extract_message_text(list_item)
                        if text:
                            print(text, end="" if streaming else "\n", flush=streaming)
                        else:
                            print(list_item, end="" if streaming else "\n", flush=streaming)
            elif isinstance(v, dict):
                for v_key, v_value in v.items():
                    print(f"{v_key}:\n{v_value}")
            else:
                if streaming:
                    print(v, end="", flush=True)
                else:
                    print(f"{ANSI_GREEN}{k}{ANSI_RESET}:\n{v}")
    elif hasattr(node_chunk, "__iter__") and not isinstance(node_chunk, str):
        try:
            for item in node_chunk:
                text = extract_message_text(item)
                if text:
                    print(text, end="" if streaming else "\n", flush=streaming)
                else:
                    print(item, end="" if streaming else "\n", flush=streaming)
        except TypeError:
            print(node_chunk, end="" if streaming else "\n", flush=streaming)
    else:
        print(node_chunk, end="" if streaming else "\n", flush=streaming)


# ============================================================
# ê·¸ë˜í”„ ì‹¤í–‰ í•¨ìˆ˜ (ë™ê¸°)
# ============================================================
def stream_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig] = None,
    node_names: Optional[List[str]] = None,
    callback: Optional[Callable[[Dict[str, Any]], None]] = None,
    stream_mode: StreamMode = "messages",
) -> Optional[Dict[str, Any]]:
    """
    LangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph: ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config: ì‹¤í–‰ ì„¤ì •
        node_names: ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ë…¸ë“œ ì¶œë ¥)
        callback: ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
        stream_mode: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ("messages", "updates", "values")

    Returns:
        Optional[Dict[str, Any]]: ìµœì¢… ê²°ê³¼
    """
    config = config or {}
    node_names = node_names or []
    prev_node = ""
    final_result: Optional[Dict[str, Any]] = None

    if stream_mode == "messages":
        for chunk_msg, metadata in graph.stream(inputs, config, stream_mode="messages"):
            curr_node = metadata["langgraph_node"]
            final_result = {"node": curr_node, "content": chunk_msg, "metadata": metadata}

            if not node_names or curr_node in node_names:
                if callback:
                    callback({"node": curr_node, "content": chunk_msg})
                else:
                    _print_node_header(curr_node, prev_node=prev_node)
                    text = extract_message_text(chunk_msg)
                    if text:
                        print(text, end="", flush=True)
                prev_node = curr_node

    elif stream_mode == "values":
        # stream_mode="values": ê° ë‹¨ê³„ì˜ ì „ì²´ ìƒíƒœë¥¼ ë°˜í™˜
        for chunk in graph.stream(inputs, config, stream_mode="values"):
            final_result = chunk
            if callback:
                callback({"content": chunk})
            else:
                # ìµœì‹  ë©”ì‹œì§€ ì¶œë ¥
                if "messages" in chunk and chunk["messages"]:
                    latest_msg = chunk["messages"][-1]
                    text = extract_message_text(latest_msg)
                    if text:
                        print(text, end="\n", flush=True)
                    # ë„êµ¬ í˜¸ì¶œ í‘œì‹œ
                    tool_calls = extract_tool_calls(latest_msg)
                    if tool_calls:
                        print(f"{ANSI_MAGENTA}Calling tools: {[tc.get('name') for tc in tool_calls]}{ANSI_RESET}")

    else:  # updates
        for namespace, chunk in graph.stream(
            inputs, config, stream_mode="updates", subgraphs=True
        ):
            for node_name, node_chunk in chunk.items():
                final_result = {"node": node_name, "content": node_chunk, "namespace": namespace}

                if node_names and node_name not in node_names:
                    continue

                if callback:
                    callback({"node": node_name, "content": node_chunk})
                else:
                    _print_node_header(node_name, namespace)
                    _print_node_chunk(node_chunk, streaming=False)
                    print(SEPARATOR_LINE)

    return final_result


def invoke_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig] = None,
    node_names: Optional[List[str]] = None,
    callback: Optional[Callable[[Dict[str, Any]], None]] = None,
) -> Optional[Dict[str, Any]]:
    """
    LangGraph ì•±ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph: ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config: ì‹¤í–‰ ì„¤ì •
        node_names: ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ë…¸ë“œ ì¶œë ¥)
        callback: ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜

    Returns:
        Optional[Dict[str, Any]]: ìµœì¢… ê²°ê³¼
    """
    return stream_graph(
        graph=graph,
        inputs=inputs,
        config=config,
        node_names=node_names,
        callback=callback,
        stream_mode="updates",
    )


# ============================================================
# ê·¸ë˜í”„ ì‹¤í–‰ í•¨ìˆ˜ (ë¹„ë™ê¸°)
# ============================================================
async def astream_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig] = None,
    node_names: Optional[List[str]] = None,
    callback: Optional[Callable] = None,
    stream_mode: StreamMode = "messages",
    include_subgraphs: bool = False,
    show_reasoning: bool = False,
) -> Dict[str, Any]:
    """
    LangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³  ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    LangChain v1.0+ì˜ create_agent ë° ìµœì‹  ìŠ¤íŠ¸ë¦¬ë° íŒ¨í„´ì„ ì§€ì›í•©ë‹ˆë‹¤.

    Args:
        graph: ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´ ë˜ëŠ” create_agentë¡œ ìƒì„±ëœ ì—ì´ì „íŠ¸
        inputs: ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config: ì‹¤í–‰ ì„¤ì • (ì„ íƒì )
        node_names: ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ë…¸ë“œ ì¶œë ¥)
        callback: ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
        stream_mode: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ("messages", "updates", "values")
        include_subgraphs: ì„œë¸Œê·¸ë˜í”„ í¬í•¨ ì—¬ë¶€
        show_reasoning: reasoning ë¸”ë¡ í‘œì‹œ ì—¬ë¶€ (ê¸°ë³¸: False)

    Returns:
        Dict[str, Any]: ìµœì¢… ê²°ê³¼

    Raises:
        ValueError: ìœ íš¨í•˜ì§€ ì•Šì€ stream_modeê°€ ì£¼ì–´ì§„ ê²½ìš°
    
    Example:
        ```python
        from langchain.agents import create_agent
        
        agent = create_agent(model="gpt-4o", tools=[...])
        result = await astream_graph(
            agent, 
            {"messages": [{"role": "user", "content": "Hello"}]},
            stream_mode="messages"
        )
        ```
    """
    config = config or {}
    node_names = node_names or []
    final_result: Dict[str, Any] = {}
    prev_node = ""

    if stream_mode == "messages":
        final_result = await _astream_messages_mode(
            graph, inputs, config, node_names, callback, prev_node, show_reasoning
        )
    elif stream_mode == "values":
        final_result = await _astream_values_mode(
            graph, inputs, config, node_names, callback
        )
    elif stream_mode == "updates":
        final_result = await _astream_updates_mode(
            graph, inputs, config, node_names, callback, include_subgraphs, prev_node
        )
    else:
        raise ValueError(
            f"Invalid stream_mode: {stream_mode}. Must be 'messages', 'updates', or 'values'."
        )

    return final_result


async def _astream_messages_mode(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str],
    callback: Optional[Callable],
    prev_node: str,
    show_reasoning: bool = False,
) -> Dict[str, Any]:
    """messages ëª¨ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    final_result: Dict[str, Any] = {}

    async for chunk_msg, metadata in graph.astream(
        inputs, config, stream_mode="messages"
    ):
        curr_node = metadata["langgraph_node"]
        final_result = {"node": curr_node, "content": chunk_msg, "metadata": metadata}

        if not node_names or curr_node in node_names:
            if callback:
                result = callback({"node": curr_node, "content": chunk_msg})
                if hasattr(result, "__await__"):
                    await result
            else:
                _print_node_header(curr_node, prev_node=prev_node)
                _print_chunk_content(chunk_msg, show_reasoning=show_reasoning)

            prev_node = curr_node

    return final_result


async def _astream_values_mode(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str],
    callback: Optional[Callable],
) -> Dict[str, Any]:
    """values ëª¨ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    final_result: Dict[str, Any] = {}

    async for chunk in graph.astream(inputs, config, stream_mode="values"):
        final_result = chunk

        if callback:
            result = callback({"content": chunk})
            if hasattr(result, "__await__"):
                await result
        else:
            # ìµœì‹  ë©”ì‹œì§€ ì¶œë ¥
            if "messages" in chunk and chunk["messages"]:
                latest_msg = chunk["messages"][-1]
                text = extract_message_text(latest_msg)
                if text:
                    print(text, end="\n", flush=True)
                tool_calls = extract_tool_calls(latest_msg)
                if tool_calls:
                    print(f"{ANSI_MAGENTA}Calling tools: {[tc.get('name') for tc in tool_calls]}{ANSI_RESET}")

    return final_result


async def _astream_updates_mode(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str],
    callback: Optional[Callable],
    include_subgraphs: bool,
    prev_node: str,
) -> Dict[str, Any]:
    """updates ëª¨ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    final_result: Dict[str, Any] = {}

    async for chunk in graph.astream(
        inputs, config, stream_mode="updates", subgraphs=include_subgraphs
    ):
        # ë°˜í™˜ í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬ ë°©ë²• ë¶„ê¸°
        if isinstance(chunk, tuple) and len(chunk) == 2:
            namespace, node_chunks = chunk
        else:
            namespace = ()
            node_chunks = chunk

        if isinstance(node_chunks, dict):
            for node_name, node_chunk in node_chunks.items():
                final_result = {
                    "node": node_name,
                    "content": node_chunk,
                    "namespace": namespace,
                }

                if node_names and node_name not in node_names:
                    continue

                if callback is not None:
                    result = callback({"node": node_name, "content": node_chunk})
                    if hasattr(result, "__await__"):
                        await result
                else:
                    _print_node_header(node_name, namespace, prev_node)
                    _print_node_chunk(node_chunk, streaming=True)

                prev_node = node_name
        else:
            print("\n" + SEPARATOR_LINE)
            print("ğŸ”„ Raw output ğŸ”„")
            print(SEPARATOR_DASH)
            print(node_chunks, end="", flush=True)
            final_result = {"content": node_chunks}

    return final_result


async def ainvoke_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig] = None,
    node_names: Optional[List[str]] = None,
    callback: Optional[Callable] = None,
    include_subgraphs: bool = True,
) -> Dict[str, Any]:
    """
    LangGraph ì•±ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph: ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config: ì‹¤í–‰ ì„¤ì • (ì„ íƒì )
        node_names: ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ë…¸ë“œ ì¶œë ¥)
        callback: ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜
        include_subgraphs: ì„œë¸Œê·¸ë˜í”„ í¬í•¨ ì—¬ë¶€

    Returns:
        Dict[str, Any]: ìµœì¢… ê²°ê³¼
    """
    return await astream_graph(
        graph=graph,
        inputs=inputs,
        config=config,
        node_names=node_names,
        callback=callback,
        stream_mode="updates",
        include_subgraphs=include_subgraphs,
    )


# ============================================================
# ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° (ìµœì‹  LangChain astream_events ì§€ì›)
# ============================================================
async def astream_events(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig] = None,
    event_types: Optional[List[str]] = None,
    callback: Optional[Callable] = None,
) -> Dict[str, Any]:
    """
    LangGraphì˜ ì´ë²¤íŠ¸ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    
    ìµœì‹  LangChainì˜ astream_events APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        graph: ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs: ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config: ì‹¤í–‰ ì„¤ì • (ì„ íƒì )
        event_types: í•„í„°ë§í•  ì´ë²¤íŠ¸ íƒ€ì… ëª©ë¡
            - "on_chat_model_start": ëª¨ë¸ ì‹œì‘
            - "on_chat_model_stream": í† í° ìŠ¤íŠ¸ë¦¬ë°
            - "on_chat_model_end": ëª¨ë¸ ì™„ë£Œ
            - "on_tool_start": ë„êµ¬ ì‹œì‘
            - "on_tool_end": ë„êµ¬ ì™„ë£Œ
        callback: ê° ì´ë²¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜

    Returns:
        Dict[str, Any]: ìµœì¢… ê²°ê³¼
    """
    config = config or {}
    final_result: Dict[str, Any] = {}
    
    async for event in graph.astream_events(inputs, config, version="v2"):
        event_type = event.get("event", "")
        
        # ì´ë²¤íŠ¸ íƒ€ì… í•„í„°ë§
        if event_types and event_type not in event_types:
            continue
        
        final_result = event
        
        if callback:
            result = callback(event)
            if hasattr(result, "__await__"):
                await result
        else:
            # ê¸°ë³¸ ì¶œë ¥
            if event_type == "on_chat_model_start":
                print(f"{ANSI_CYAN}[Model Start]{ANSI_RESET} Input: {event.get('data', {}).get('input', '')[:50]}...")
            
            elif event_type == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk:
                    text = extract_message_text(chunk)
                    if text:
                        print(text, end="", flush=True)
            
            elif event_type == "on_chat_model_end":
                output = event.get("data", {}).get("output")
                if output:
                    print(f"\n{ANSI_GREEN}[Model End]{ANSI_RESET}")
            
            elif event_type == "on_tool_start":
                tool_name = event.get("name", "unknown")
                print(f"\n{ANSI_MAGENTA}[Tool Start]{ANSI_RESET} {tool_name}")
            
            elif event_type == "on_tool_end":
                tool_name = event.get("name", "unknown")
                output = event.get("data", {}).get("output", "")
                print(f"{ANSI_MAGENTA}[Tool End]{ANSI_RESET} {tool_name}: {str(output)[:100]}...")

    return final_result
